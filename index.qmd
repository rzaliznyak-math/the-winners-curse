---
title: "The Winner's Curse"
subtitle: "Significance Testing is Not an Endorsement of the Treatment Effect"
author: "Russ Zaliznyak <[rzaliznyak@gmail.com](mailto:rzaliznyak@gmail.com)>"
date: "2025-12-01"
execute:
  echo: false
format: 
  html: 
    css: style.css
    toc: true
    toc-expand: true
    toc-indent: 1em
---

# Introduction

You did everything correctly with your AB Test --- determined your required sample size in advance, collected all of your samples, and calculated a p-value = 0.005 at the end of your experiment.

Even with such a low p-value, _Significance Testing_ is not an endorsement of the observed treatment effect! Using simulations below, I'll explain why this is always the case.




# AB Test Setup

Let's design a properly-powered _Significance Test_. 

## Baseline
Let's assume our baseline metric floats around 65%.

```{python}
#| echo: true
#| code-fold: false
control_mean = 0.65
```

## Significance Threshold
We use a traditional _Significance Threshold_ of 0.05. If our test conditions are equal, 5% of the time we will accidentally and falsely detect a signal.
```{python}
#| echo: true
#| code-fold: false
alpha = 0.05
```


## Minimum Detectable Effect
We set a _Minimum Detectable Effect_ = 101. Our Significance Test will be designed to reliably detect a difference --- assuming this _MDE_ is actually present in our treatment condition.

```{python}
#| echo: true
#| code-fold: false
mde = 101
index_to_control_planned = mde / 100
```



## Statistical Power
We use a traditional _Statistical Power_ of 0.80. If our _MDE_ is true, our experiment has an 80% chance of detecting statistical significance.

```{python}
#| echo: true
#| code-fold: false
power = 0.80
```

## Required Samples

```{python}
#| code-summary: "<b>How to Calculate Required Samples</b>"
#| echo: true
#| code-fold: true


from math import ceil, sqrt
from scipy.stats import norm


def two_proportion_required_samples(
    alpha,
    power,
    control_rate,
    treatment_rate,
    control_to_treatment_ratio=1,
    tail_type="one_tail",
):
    """Calculate the required number of samples for two-proportion test.
    :param float alpha: Value in (0,1) that specifies desired False Positive Rate
    :param float power: Value in (0,1) that specifies desired 1 - False Negative Rate
    :param float control_rate: Value in (0,1) that specifies expected control rate
    :param float treatment_rate: Value in (0,1) that specifies expected treatment rate
    :param float control_to_treatment_ratio: The ratio of control to treatment samples
    :param string tail_type: Specifies one-tail or two-tail experiment\n
        Defaults to "two_tail" if anything other than "one_tail" is given
    :return: prob_tuple: Tuple of required control samples and treatment samples
    :rtype: tuple
    """
    alpha_adjusted = (
        alpha
        if tail_type is None
        or tail_type.lower() == "one_tail"
        else alpha / 2
    )
    beta = 1 - power

    ##Determine how extreme z-score must be to reach statistically significant result
    z_stat_critical = norm.ppf(1 - alpha_adjusted)
    ##Determine z-score of Treatment - Control that corresponds to a True Positive Rate (1-beta)
    z_power_critical = norm.ppf(beta)
    ##Expected Difference between treatment and control
    expected_delta = treatment_rate - control_rate
    ##Control Allocation Rate
    control_allocation = (
        control_to_treatment_ratio
        / (control_to_treatment_ratio + 1)
    )
    ##Treatment Allocation Rate
    treatment_allocation = 1 - control_allocation

    ##Calculate Variance of Treatment Rate - Control Rate
    blended_p = (
        treatment_rate * treatment_allocation
        + control_rate * control_allocation
    )
    blended_q = 1 - blended_p
    variance_blended = (
        blended_p
        * blended_q
        / (
            control_allocation
            * treatment_allocation
        )
    )
    ##Total Samples Required
    total_samples_required = (
        variance_blended
        * (
            (z_power_critical - z_stat_critical)
            / (0 - expected_delta)
        )
        ** 2
    )
    ##Split total samples into control and treatment
    control_samples = ceil(
        control_allocation
        * total_samples_required
    )
    treatment_samples = ceil(
        treatment_allocation
        * total_samples_required
    )
    ##Perhaps return required_delta in the future
    required_delta = (
        z_stat_critical * sqrt(variance_blended)
        + 0
    )
    return (control_samples, treatment_samples)



```

